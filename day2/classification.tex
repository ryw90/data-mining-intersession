% source for day 2 slides

\documentclass{beamer}

\usepackage{amsmath}
\usepackage{graphicx}

\title{Introduction to Data Mining}
\author{Scott Powers and Ryan Wang}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
Classification
\end{frame}

\begin{frame}
\frametitle{Introduction to Statistical Learning}
\includegraphics[scale = 0.4]{isl.jpg}\\~\\
Available as a pdf for free online (legally)!\\~\\
Today: Chapter 4
\end{frame}

\begin{frame}
\frametitle{Classification}
Yesterday: Regression\\
Goal: Predict a {\bf quantitative} response\\~\\
Today: Classification\\
Goal: Predict a {\bf qualitative} response
\end{frame}

\begin{frame}
\frametitle{RMS Titanic}
\begin{center}
\includegraphics[scale = .15]{titanic.jpg}
\end{center}
\begin{itemize}
\item In 1912, the Titanic sank after colliding with an iceberg
\item Total number of passengers and crew: 2224
\begin{itemize}
\item How many people survived? \pause 722
\end{itemize}
\item Kaggle: Can we predict which passengers survived?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Data}
{\small train.csv:}
\begin{center}
{\tiny
\begin{tabular}{|r|r|r|l|r|r|r|r|r|l|}\hline
ID	& Surv.	& Cl	& Name					& S	& Age	& Sib/Sp	& Par/Ch	& Fare	& Port		\\\hline
1	& 0		& 3	& Braund, Owen			& M	& 22		& 1		& 0		& \$7.25	& Southampton	\\\hline
2	& 1		& 1	& Cumings, Florence		& F	& 38		& 1		& 0		& \$71.28	& Cherbourg	\\\hline
3	& 1		& 3	& Heikkinen, Laina			& F	& 26		& 0		& 0		& \$7.93	& Southampton	\\\hline
4	& 1		& 1	& Futrelle, Lily				& F	& 35		& 1		& 0		& \$53.10	& Southampton	\\\hline
5	& 0		& 3	& Allen, William			& M	& 35		& 0		& 0		& \$8.05	& Southampton	\\\hline
6	& 0		& 3	& Moran, James			& M	& NA		& 0		& 0		& \$8.46	& Queenstown	\\\hline
...	& ...		& ...	& ...						& ...	& ...		& ...		& ...		& ...		& ...			\\\hline
891	& 0		& 3	& Dooley, Patrick			& M	& 32		& 0		& 0		& \$7.75	& Queenstown	\\\hline
\end{tabular}
}
\end{center}
{\small test.csv:}
\begin{center}
{\tiny
\begin{tabular}{|r|r|r|l|r|r|r|r|r|l|}\hline
ID	& Surv.	& Cl	& Name					& S	& Age	& Sib/Sp	& Par/Ch	& Fare	& Port		\\\hline
892	& ?		& 3	& Kelly, James				& M	& 35		& 0		& 0		& \$7.83	& Queenstown	\\\hline
893	& ?		& 3	& Wilkes, Ellen				& F	& 47		& 1		& 0		& \$7.00	& Southampton	\\\hline
894	& ?		& 2	& Myles, Thomas\hspace{.25cm} & M& 62	& 0		& 0		& \$9.69	& Queenstown	\\\hline
...	& ...		& ...	& ...						& ...	& ...		& ...		& ...		& ...		& ...			\\\hline
1309	& ?		& 3	& Peter, Michael			& M	& NA		& 1		& 1		& \$22.36	& Cherbourg	\\\hline
\end{tabular}
}
\end{center}
\vspace{.5cm}
{\small Brainstorm: What features {\it might} make a person more likely to survive?}
\end{frame}

\begin{frame}
\frametitle{Method 1: $k$-Nearest Neighbors}
1-Nearest Neighbor: For each person in the test set,\\
\hspace{2.5cm} find the most similar person in the training set.\\~\\
2-Nearest Neighbors: For each person in the test set,\\
\hspace{2.5cm} find the 2 most similar people in the training set.\\~\\
3-Nearest Neighbors: For each person in the test set,\\
\hspace{2.5cm} find the 3 most similar people in the training set.\\
...\\~\\
$k$-Nearest Neighbors: For each person in the test set,\\
\hspace{2.5cm} find the $k$ most similar people in the training set.\\~\\~\\
How do we define similarity?
\end{frame}

\begin{frame}
\frametitle{$k$-Nearest Neighbors on a toy data set}
\begin{center}
\includegraphics[scale=0.4]{plot.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{$k$-Nearest Neighbors on Titanic data}
\end{frame}

\begin{frame}
\frametitle{Method 2: Linear Regression}
What happens if we apply linear regression to these data?\\
\pause\vspace{2cm}
How do we interpret this output?\\
\pause\vspace{2cm}
How does this prediction compare with $k$-NN?
\end{frame}

\begin{frame}
\frametitle{Problems with linear regression as a classification method}
-- Interpretability\\
\vspace{1cm}
-- Assumption: $y_i \sim \mathcal N(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}, \sigma^2)$\\~\\
We would like to see something along the lines of:
$$y_i = \left\{\begin{array}{ll}
	1	& \mbox{ with probability }\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}\\\\
	0	& \mbox{ with probability }1 - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip})
\end{array}\right.$$
\\\vspace{1cm}
-- Also: difficult to extend beyond two classes
\end{frame}

\begin{frame}
\frametitle{Method 3: Logistic Regression}
$$y_i = \left\{\begin{array}{ll}
	1	& \mbox{ with probability }\frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}}}\\\\
	0	& \mbox{ with probability }1 - \frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}}}
\end{array}\right.$$
\begin{center}
\includegraphics[scale = 0.3]{logistic.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{How to fit the logistic regression model}
Using ``least squares'' to find $\beta$'s doesn't make sense\\
{\small In {\tt R}: Simply replace {\tt lm(...)} with {\tt glm(..., family = binomial)}}
\end{frame}

\begin{frame}
\frametitle{Method 4: Linear Discriminant Analysis}
\end{frame}

\begin{frame}
\frametitle{Method 5: Quadratic Discriminant Analysis}
\end{frame}

\end{document}